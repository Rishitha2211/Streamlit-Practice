# -*- coding: utf-8 -*-
"""TestMLExplorerApp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_P_YEle_MTb7teBItNk4kZ8sSYjTOJ_
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# Function to load dataset
def load_data(dataset_name):
    if dataset_name == "Iris":
        from sklearn.datasets import load_iris
        data = load_iris()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
    elif dataset_name == "Titanic":
        df = pd.read_csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")
        df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']]
        df = df.dropna()
        le = LabelEncoder()
        df['Sex'] = le.fit_transform(df['Sex'])
        df['Embarked'] = le.fit_transform(df['Embarked'])
        df.rename(columns={'Survived': 'target'}, inplace=True)
    return df

# Streamlit UI
st.title("Machine Learning Model Explorer")

# Dataset Selection
dataset_name = st.selectbox("Select Dataset", ["Iris", "Titanic"])
df = load_data(dataset_name)

st.write("### Dataset Preview")
st.dataframe(df.head())

# Splitting data
X = df.drop(columns=['target'])
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Selection
model_name = st.selectbox("Select Model", ["Random Forest", "XGBoost", "Decision Tree", "Logistic Regression"])

# Configure model hyperparameters
if model_name == "Random Forest":
    n_estimators = st.slider("Number of Trees", 10, 200, step=10, value=100)
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
elif model_name == "XGBoost":
    learning_rate = st.slider("Learning Rate", 0.001, 0.2, step=0.01, value=0.1)
    n_estimators = st.slider("Number of Trees", 10, 200, step=10, value=100)
    model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=42, eval_metric='logloss')
elif model_name == "Decision Tree":
    max_depth = st.slider("Max Depth", 1, 50, step=1, value=5)
    model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
elif model_name == "Logistic Regression":
    model = LogisticRegression()

# Train model
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

st.write(f"### Model Performance: Accuracy = {accuracy:.2f}")

# Display Classification Report
st.write("### Classification Report")
st.text(classification_report(y_test, y_pred))

# Confusion Matrix
st.write("### Confusion Matrix")
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(3, 3))  # Reduce size
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
st.pyplot(fig)

# Feature Importance (for tree-based models)
if model_name in ["Random Forest", "XGBoost", "Decision Tree"]:
    st.write("### Feature Importance")
    feature_importances = model.feature_importances_
    fi_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
    fi_df = fi_df.sort_values(by='Importance', ascending=False)
    fig, ax = plt.subplots(figsize=(4, 3))  # Reduce size
    sns.barplot(x='Importance', y='Feature', data=fi_df, ax=ax)
    st.pyplot(fig)
st.markdown("<small>This app is inspired by Soumya Kushwaha's ML Model Explorer.</small>", unsafe_allow_html=True)